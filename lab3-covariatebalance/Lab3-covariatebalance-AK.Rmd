---
title: "POLI 787 - Covariate Balancing"
author: "Colin Case"
date: "September 1, 2022"
output: pdf_document
header-include:
  - \usepackage{amsbsy}
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \DeclareMathOperator*{\argmin}{argmin} % thin space, limits underneath in displays
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE)
```


# Covariate Balance

When trying to estimate quantities of interest, such as the population average treatment effect (PATE), we are focused on making sure our treatment is random. In other words, we are trying to achieve (conditional) ignorability where the treatment status is independent of the potential outcomes (or in the case of conditional ignorability, independent conditional on some variables $X_i$). We can think about this more tangibly in two ways: the potential outcomes for both the treatment and control group should have the same distribution, and the treatment and control groups are *balanced* on average when considering observable characteristics such as other covariates (we obviously cannot balance on unobservable characteristics). 

Today, we are going to focus on testing for covariance balance (in the next few weeks, we will also look at ways to achieve covariate balance when a treatment status is not purely randomized). When conducting these sorts of tests, we need to consider what we are actually testing. If our goal is to prove balance, our hypothesis testing should represent as such; rather, the current practice in political science is to use a null hypothesis of "no difference" and equates failing to reject the null with covariate balance. Recent work by [Hartman and Hidalgo (2018)](https://doi.org/10.1111/ajps.12387) has taken a contrary stance, instead arguing that the difference should be the null and equivalence should be the alternative. 

In lab today, we are going to consider two natural experiments discussed in Hartman and Hidalgo's paper where authors conducted covariate balance testing: [Brady and McNulty (2011)](https://doi.org/10.1017/S0003055410000596) which uses a natural experiment to look at the effect of distance to the polling place on turnout, and [Dunning and Nilekani (2013)](https://doi.org/10.1017/S0003055412000573) which uses a natural experiment to evaluate the effect of ethnic quotas on redistribution.

## Example: Brady and McNulty (2011)

Brady and McNulty are looking at the "as-if" random consolidation of voting places in Los Angeles. For their design to have a valid causal identification strategy, it is important that the distance to the polling station before consolidation is the same for treatment and control groups. In the original paper, the authors assume a null hypothesis that covariates are balanced and an alternative that they are not. However, given the large sample size, the difference is highly significant but substantively small. Let's see how equivalence testing could have made their case stronger.

Unfortunately, no replication data is public for Brady and McNulty, but Hartman and Hidalgo do collect enough summary information to conduct an equivalence test by hand. The information you will need to do this can be found here:

- "Those who had their polling place changed in 2003 had to go an average distance of 0.354 miles in 2002, those who did not have their polling place changed had to go only 0.320 milesâ€”a difference of 0.034 miles"
- From figure 1 -- 3045206 voters, assuming roughly equal split between treatment and control
- From figure 1 -- pooled std dev (assuming equal) of 0.2772

To start, specify the difference in means (dbar), sample size for each treatment status (m and n), the total sample, and the variances for each treatment status (x.var and y.var, which are equal). We are first going to conduct a simple difference of means test (because we don't have a dataframe with individual observations, we can do this by hand)

```{r}
# Clear environment
rm(list = setdiff(ls(), lsf.str()))

# Set values from above
dbar <- 0.034 # Mean difference between treatment and control for distance
m <- (3045206)/2 # Number in treatment
n <- (3045206)/2 # Number in control
N <- m+n # Total Sample Size
x.var <-  (.2772)^2 # Calculate variance from pooled standard deviation (treatment)
y.var <- (.2772)^2 # Calculate variance from pooled standard deviation (control)

# Difference in means test by hand
se <- sqrt( (1/m + 1/n) * ((m-1)*x.var^2 + (n-1)*y.var^2)/(N-2)) # Standard Error
t <- (dbar)/se # T-stat
df <- N-2 # Degrees of Freedom
dat <- c(dbar, se, t, 2*pt(-abs(t),df)) # Values related to test
names(dat) <- c("Difference of means", "Std Error", "t", "p-value") 
dat # Display values


```

From their data, it is clear there is a statically significant difference in distance to polling station by treatment status. However, this difference could be argued is substantively small. If you were to be picky, you could argue they need to balance on their coveriates to really get at the causal effect of the treatment status. However, a better test would be to compare the two values and test for similarity.


For our test, we are going to use an equivalence interval of about 0.2 standard deviations. This is a commonly accepted standard and seen as a strict test. Using all of the information we have to this point, we can calculate a t-test. Our null hypothesis is 2002 distance to polling stations is different for the treatment and control conditions. Our alternative hypothesis is they are not different. We will also include a confidence interval at the $\alpha = 0.05$ level. The resulting value can be interpreted as the smallest equivalence interval supported by the data given the observed difference between treatment conditions.

```{r}
## using very strict tolerance of 0.2
epsilon <- 0.2

# T Statistic for Equivalence Test
t.stat <- sqrt(m*n * (N-2)/N) * dbar / sqrt((m - 1)*x.var + (n - 1)*y.var)
t.stat

# Set Equivalence Confidence Interval
alpha <- 0.05

# equiv confidence interval
inverted <- uniroot(function(x) pf(abs(t.stat)^2, 1, N-2, ncp = (m*n*x^2)/N)
                    - alpha, c(0,2*abs(t.stat)), tol = 0.0001)$root
inverted

```

As you can see, the t-statistic from our calculation is quite large, suggesting there is not a difference between these two groups on pre-treatment distance to the polling station. While Brady and McNulty had their work published, it is easy to see how potentially using the wrong test could have derailed their paper! The resulting confidence interval is only 0.124 standard deviations of pre-treatment difference, or 0.035 miles.

## Excercise: Dunning and Nilekani (2013)

The second example used in Hartman and Hidalgo compares equivalence testing with traditional balance tests that were used in Dunning and Nilekani. This time, instead of working with summary statistics, we will work with the original dataset and see how these tests can be applied. 

```{r}
# Clear environment
rm(list = setdiff(ls(), lsf.str()))

# Set WD
#setwd("C:/Users/colin/poli787-fall22/lab3-covariatebalance")

# Load data
load('dunning_nilekani_replication.Rdata')

# Load Packages
```

In this dataset, our treatment variable is `scst_reserved_current`, a binary variable for whether or not a council had reserved quotas for certain ethnic groups or not. The dataset `dunning_nilekani_replication.Rdata` contains all the covariates used in Dunning and Nilekni (see Table 2). We are first going to start with the same test used in the paper. In this scenario, our null hypothesis is that covariates are balanced across treatment and control conditions and the alternative hypothesis is that they are different. To do this test, we are going to run a t-test on the balance covariates by treatment status.

```{r}
data <- data[,1:9] # Restrict data to variables in table 2 of paper
treatment <- subset(data, scst_reserved_current == 1) # Create treatment DF
control <- subset(data, scst_reserved_current == 0) # Create control DF

for(i in 2:9){ # Create loop for all pre-treatment covariates
  temp <- t.test(treatment[,i], control[,i]) # t.test for each covariate
  cat('The difference of means for', colnames(data)[i], 'is', # Display difference
     temp[["estimate"]][["mean of x"]]-temp[["estimate"]][["mean of y"]], '\n')
  cat('The p-value for',  colnames(data)[i], 'is', temp$p.value, '\n') # Display p-value
}




```
Take a look at the p-values in the present section and recall that our null hypothesis is that these values are the same in the t-test. How would you interpret these results? Like all hypothesis testing, we would only fail to reject the null and cannot accept the null. But in many papers that use t-tests for covariate balance, that is exactly what we are doing! When we look ata few p-values in particular (No_HH stands out), how would you handle a p-value of 0.09? This is why equivilence testing is important. 

Beyond a simple t-test, we can also perform something called a Kolmogorov-Smirnov test. In the two-sample test, we are testing to see if two samples (here the treatment and control) came from the same distribution.

```{r}

for(i in 2:9){ # Create loop for all pre-treatment covariates
  temp <- ks.test(treatment[,i], control[,i]) # K-S test for Each covariate
  cat('The p-value for',  colnames(data)[i], 'is', temp$p.value, '\n') # Display p-value
}

```

Again, we get values that do not lead us to conclude that the distributions are different, but also do not allow us to conclude that the distributions are the same.

We are now going to turn to conducting an equivalence test using `t_Tost`. It should be noted, this is one of a few ways to conduct this test depending on the data that you have. We are going to focus on one of the more problematic covariates, `No_HH`. For our test, we will use the same range as that used in Hartman and Hidalgo, which is $0.36\sigma$. You can then plot the results as well as view the summary output. What should the authors have concluded about covariate balance in the original paper?

```{r}
# Load Package
library(TOSTER)
# Conduct Equivalence Test Using T_Tost
e.test <- t_TOST(formula = No_HH ~ scst_reserved_current, # Specify covariate ~ treatment status
       data = data, # Call data
       alpha = .05, # Alpha level
       low_eqbound = -.36*sd(data$No_HH , na.rm = TRUE), # Set LB and UB (raw number)
       high_eqbound = .36*sd(data$No_HH , na.rm = TRUE))
# Plot Test
plot(e.test)
# Print Summary Information
print(e.test)
```



